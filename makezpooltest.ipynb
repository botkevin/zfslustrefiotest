{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\r\n",
    "import os\r\n",
    "import csv\r\n",
    "import runtests"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def return_one(startdisk, zfsname, csvfilename, ip, skip_num=0):\r\n",
    "    with open(csvfilename) as csvfile:\r\n",
    "        reader = csv.reader(csvfile, delimiter=\",\")\r\n",
    "        next(reader)\r\n",
    "        for _ in range(skip_num):\r\n",
    "            next(reader)\r\n",
    "        row = next(reader)\r\n",
    "        testname, numberdisks, raidmode, raid0, ashift, compression, recordsize, atime, filesize, benchmark, runtime, blocksizes, iodepths, numjobs = row\r\n",
    "        print (testname, numberdisks, raidmode, raid0, ashift, compression, recordsize, atime, filesize, benchmark, runtime, blocksizes, iodepths, numjobs)\r\n",
    "        print (\"++++++++++++++++++++++++++++\")\r\n",
    "        print (testname)\r\n",
    "        print (\"++++++++++++++++++++++++++++\")\r\n",
    "        numberdisks = int(numberdisks)\r\n",
    "        raidmode = int(raidmode)\r\n",
    "        raid0 = int(raid0)\r\n",
    "        return runtests.make_commands(startdisk, numberdisks, zfsname, raidmode, raid0, ashift, compression, recordsize, atime, testname, ip, filesize, benchmark, runtime, blocksizes, iodepths, numjobs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "cmds = return_one(\"c\", \"tank\", \"config.csv\", \"192.168.169.207\", skip_num=6)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "z2_one_8_single 8 6 0 12  32m off 500g True 200 16m 32m 64m 1 2 4 1 4 8\n",
      "++++++++++++++++++++++++++++\n",
      "z2_one_8_single\n",
      "++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "for cmd in cmds:\r\n",
    "    print(cmd)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mkdir z2_one_8_single\n",
      "echo -------------------------------\n",
      "echo make zfs\n",
      "echo -------------------------------\n",
      "zpool create tank -o ashift=12 raidz2 sdc sdd sde sdf sdg sdh sdi sdj\n",
      "echo -------------------------------\n",
      "echo test zfs\n",
      "echo -------------------------------\n",
      "./bench_fio  --type directory --quiet -m write read --loops 1 --target /tank -o z2_one_8_single_zfs -b 16m 32m 64m --iodepth 1 2 4 --numjobs 1 4 8 --size 500g --runtime 200\n",
      "echo -------------------------------\n",
      "echo make lustre\n",
      "echo -------------------------------\n",
      "mkfs.lustre --backfstype=zfs --fsname=test --index=0 --reformat --mgs --mdt mgsmdt/mgsmdt\n",
      "mkfs.lustre --backfstype=zfs --fsname=test --index=0 --ost --mgsnode=192.168.169.207@tcp tank/ost0\n",
      "mount -t lustre mgsmdt/mgsmdt /mnt/mgsmdt\n",
      "mount -t lustre tank/ost0 /mnt/ost\n",
      "mount -t lustre 192.168.169.207@tcp:/test /mnt/lustre\n",
      "echo -------------------------------\n",
      "echo test lustre\n",
      "echo -------------------------------\n",
      "./bench_fio  --type directory --quiet -m write read --loops 1 --target /mnt/lustre -o z2_one_8_single_lustre -b 16m 32m 64m --iodepth 1 2 4 --numjobs 1 4 8 --size 500g --runtime 200\n",
      "echo -------------------------------\n",
      "echo cleanup\n",
      "echo -------------------------------\n",
      "umount -f 192.168.169.207@tcp:/test\n",
      "umount mgsmdt/mgsmdt\n",
      "umount tank/ost0\n",
      "tunefs.lustre --writeconf mgsmdt/mgsmdt\n",
      "tunefs.lustre --writeconf tank/ost0\n",
      "zpool destroy -f tank\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# csv stuff is in makezpool.py"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# STATICS\r\n",
    "RAID_CONFIGS = {\r\n",
    "    5:\"raidz\",\r\n",
    "    6:\"raidz2\",\r\n",
    "    7:\"raidz3\",\r\n",
    "    0:\"\",\r\n",
    "    1:\"mirror\"\r\n",
    "}\r\n",
    "\r\n",
    "ALPHABET = alphabets=[str(chr(ord('a')+i)) for i in range(26)]\r\n",
    "\r\n",
    "# not really a static but we will just do this for now\r\n",
    "SKIP_ZFS_TESTS = False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# parameters\r\n",
    "startdisk_ = \"c\"\r\n",
    "numberdisks_ = 2\r\n",
    "zfsname_ = \"tank\"\r\n",
    "raidmode_ = 0\r\n",
    "# do we want the raid in +0 config, \r\n",
    "#   IE if we want raid50 with 2 raid 5 volumes in raid 0 config,\r\n",
    "#   we have raidmode=5, raid0=2\r\n",
    "raid0_ = 0\r\n",
    "\r\n",
    "# zfs-options\r\n",
    "ashift_ = \"12\"\r\n",
    "compression_ = None # this may or may not work with our datasets, perhaps we can try lz4\r\n",
    "recordsize_ = \"32m\"\r\n",
    "atime_ = \"off\"\r\n",
    "\r\n",
    "testname_ = \"onedisk\"\r\n",
    "filesize_ = \"100g\"\r\n",
    "runtime_ = \"200\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def make_zfs(startdisk, numberdisks, zfsname, raidmode, raid0, ashift, compression, recordsize, atime):\r\n",
    "    raidmodestr = raidmode if type(raidmode)==str else RAID_CONFIGS[raidmode]\r\n",
    "    cmdstr = \"zpool create \" + zfsname\r\n",
    "\r\n",
    "    propertiesstr = \" -o ashift=\"+ashift\r\n",
    "    # we will worry about recordsize and atime later\r\n",
    "    # each property has to have its own \"-o\"\r\n",
    "    # this doesnt seem to work anyways, we need to set with \"zfs set\"\r\n",
    "    #  + \" recordsize=\"+recordsize + \" atime=\"+atime\r\n",
    "    # if compression:\r\n",
    "    #     propertiesstr += \" compression=\"+compression\r\n",
    "\r\n",
    "    disks = []\r\n",
    "    start = ord(startdisk[-1]) - ord('a')\r\n",
    "    for i in range(start, start + numberdisks):\r\n",
    "        j = i//26 - 1\r\n",
    "        start = \"\" if j<0 else ALPHABET[j]\r\n",
    "        disks.append (\"sd\" + start+ALPHABET[i%26])\r\n",
    "    disksstr = \"\"\r\n",
    "    for i, disk in enumerate(disks,0):\r\n",
    "        if raid0!=0 and i%(numberdisks//raid0)==0 and i != 0:\r\n",
    "            disksstr += \" \" + raidmodestr\r\n",
    "        disksstr += \" \" + disk\r\n",
    "    \r\n",
    "    cmdstr += propertiesstr\r\n",
    "    cmdstr += \" \" + raidmodestr\r\n",
    "    cmdstr += disksstr\r\n",
    "    return cmdstr\r\n",
    "\r\n",
    "def make_fio_thruput(dir, testname, filesize, runtime, fs=\"zfs\"):\r\n",
    "    # writecmd = \"fio --directory=\"+ dir                     +\" --direct=1 --rw=write --bs=32m --ioengine=libaio --iodepth=64 --filesize=\"+filesize+\" --runtime=\"+runtime+\" --time_based --group_reporting --name=throughput-test --eta-newline=1 >> \" + testname + \"/\" + fs + \"_write.txt\"\r\n",
    "    # readcmd  = \"fio --filename=\"+ dir + \"/throughput-test.0.0 --direct=1 --rw=read --bs=32m --ioengine=libaio --iodepth=64 --filesize=\"+filesize+\" --runtime=\"+runtime+\" --time_based --group_reporting --name=throughput-test --eta-newline=1 --readonly >> \" + testname + \"/\" + fs + \"_read.txt\"\r\n",
    "    writecmd = \"fio --directory=\"+ dir                     +\" --direct=1 --rw=write --bs=32m --ioengine=libaio --iodepth=64 --filesize=\"+filesize+\" --group_reporting --name=throughput-test --eta-newline=1 >> \" + testname + \"/\" + fs + \"_write.txt\"\r\n",
    "    readcmd  = \"fio --filename=\"+ dir + \"/throughput-test.0.0 --direct=1 --rw=read --bs=32m --ioengine=libaio --iodepth=64 --filesize=\"+filesize+\" --group_reporting --name=throughput-test --eta-newline=1 --readonly >> \" + testname + \"/\" + fs + \"_read.txt\"\r\n",
    "    rmcmd = \"rm \"+ dir +\"/throughput-test.0.0\"\r\n",
    "    return writecmd, readcmd, rmcmd\r\n",
    "\r\n",
    "# need to make sure that:\r\n",
    "# I don't know if this is needed? see \r\n",
    "# /etc/ldev.conf is changed:\r\n",
    "# hostname - mgs     zfs:lustre-mgs/mgs\r\n",
    "# hostname - mdt0    zfs:lustre-mdt0/mdt0\r\n",
    "# hostname - ost0    zfs:lustre-ost0/ost0\r\n",
    "# SELinux is disabled: sed -i '/^SELINUX=/s/.*/SELINUX=disabled/' /etc/selinux/config \r\n",
    "# see the vms for what we did with ldev.conf for remote connections\r\n",
    "#\r\n",
    "# lustre is started:\r\n",
    "# systemctl start lustre\r\n",
    "# ost dir should not \r\n",
    "def make_lustre(zfsname, mgsmdtname=\"mgsmdt\", ip=\"$(hostname)\", protocol=\"tcp\"):\r\n",
    "    #fs name is test btw\r\n",
    "    full_ip = ip +\"@\"+ protocol\r\n",
    "    cmds = []\r\n",
    "    cmds.append(\"mkfs.lustre --backfstype=zfs --fsname=test --index=0 --reformat --mgs --mdt \"+ mgsmdtname +\"/mgsmdt\") # mgsmdt should be static\r\n",
    "    cmds.append(\"mkfs.lustre --backfstype=zfs --fsname=test --index=0 --ost --mgsnode=\"+ full_ip + \" \" + zfsname +\"/ost0\")\r\n",
    "    # mount\r\n",
    "    cmds.append(\"mount -t lustre \" + mgsmdtname +\"/mgsmdt /mnt/mgsmdt\")\r\n",
    "    cmds.append(\"mount -t lustre \" + zfsname +\"/ost0 /mnt/ost\")\r\n",
    "    cmds.append(\"mount -t lustre \" + full_ip + \":/test\" + \" /mnt/lustre\")\r\n",
    "\r\n",
    "    # don't need this because its all mounted already. Should we remount? probably don't need to remount, but might as well... \r\n",
    "    # lustre client mount \r\n",
    "    # mount -t lustre <MGS node>\r\n",
    "    #     /<fsname> <mount point>\r\n",
    "    # ex:  mount -t lustre 192.168.169.207@tcp:/test /mnt/lustre\r\n",
    "    return cmds\r\n",
    "\r\n",
    "def clean(zfsname, mgsmdtname=\"mgsmdt\", ip=\"$(hostname)\", protocol=\"tcp\"):\r\n",
    "    full_ip = ip +\"@\"+ protocol\r\n",
    "    devices = [mgsmdtname +\"/mgsmdt\", zfsname +\"/ost0\"]\r\n",
    "    cmds = []\r\n",
    "    # lustre\r\n",
    "    cmds.append(\"umount -f \"+ full_ip + \":/test\")\r\n",
    "    for device in devices:\r\n",
    "        cmds.append(\"umount \"+ device)\r\n",
    "    for device in devices:\r\n",
    "        cmds.append(\"tunefs.lustre --writeconf \" + device)\r\n",
    "    # zfs\r\n",
    "    cmds.append (\"zpool destroy -f \"+ zfsname)\r\n",
    "    return cmds\r\n",
    "    \r\n",
    "def make_echo(msg):\r\n",
    "    line = \"echo -------------------------------\"\r\n",
    "    return line, \"echo \"+msg, line\r\n",
    "\r\n",
    "def print_title(msg):\r\n",
    "    line = \"-------------------------------\"\r\n",
    "    print (\" \"+line, \"\\n  \", msg, \"\\n\", line)\r\n",
    "\r\n",
    "def make_commands(startdisk, numberdisks, zfsname, raidmode, raid0, ashift, compression, recordsize, atime, testname, ip, filesize, runtime):\r\n",
    "    zfsdir = \"/\"+zfsname\r\n",
    "    cmds = []\r\n",
    "    cmds.append (\"mkdir \" + testname)\r\n",
    "    \r\n",
    "    # print_title (\"make zfs\")\r\n",
    "    cmds.extend (make_echo(\"make zfs\"))\r\n",
    "    cmds.append (make_zfs(startdisk, numberdisks, zfsname, raidmode, raid0, ashift, compression, recordsize, atime))\r\n",
    "    \r\n",
    "    if not SKIP_ZFS_TESTS:\r\n",
    "        # print_title (\"test zfs\")\r\n",
    "        cmds.extend (make_echo(\"test zfs\"))\r\n",
    "        cmds.extend (make_fio_thruput(zfsdir, testname, filesize, runtime, fs=\"zfs\"))\r\n",
    "\r\n",
    "    # print_title (\"make lustre\")\r\n",
    "    cmds.extend (make_echo(\"make lustre\"))\r\n",
    "    lustrecmds = make_lustre(zfsname, ip=ip)\r\n",
    "    cmds.extend (lustrecmds)\r\n",
    "\r\n",
    "    cmds.extend (make_fio_thruput(\"/mnt/lustre\", testname,filesize, runtime, fs=\"lustre\"))\r\n",
    "\r\n",
    "    cmds.extend (clean(zfsname, ip=ip))\r\n",
    "    return cmds"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "./bench_fio --target /tank --type directory -o onedisk_fioplot_fine -b 32m -m write read --loops 1 --iodepth 1 --numjobs 1 --size 50gb --runtime 200 --quiet"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_22400/3324412632.py, line 1)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\kevishi\\AppData\\Local\\Temp/ipykernel_22400/3324412632.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    ./bench_fio --target /tank --type directory -o onedisk_fioplot_fine -b 32m -m write read --loops 1 --iodepth 1 --numjobs 1 --size 50gb --runtime 200 --quiet\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def run_spec(startdisk, zfsname, csvfilename, ip, line):\r\n",
    "    with open(csvfilename) as csvfile:\r\n",
    "        reader = csv.reader(csvfile, delimiter=\",\")\r\n",
    "        line += 2\r\n",
    "        testname, numberdisks, raidmode, raid0, ashift, compression, recordsize, atime, filesize, runtime = [None]*10\r\n",
    "        for _ in range(line):\r\n",
    "            testname, numberdisks, raidmode, raid0, ashift, compression, recordsize, atime, filesize, runtime  = next(reader)\r\n",
    "        numberdisks = int(numberdisks)\r\n",
    "        raidmode = int(raidmode)\r\n",
    "        raid0 = int(raid0)\r\n",
    "        print (testname)\r\n",
    "        return make_commands(startdisk, numberdisks, zfsname, raidmode, raid0, ashift, compression, recordsize, atime, testname, ip, filesize, runtime)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cmds = run_spec(startdisk_, zfsname_, \"config.csv\", \"192.168.169.207\", 0)\r\n",
    "for cmd in cmds:\r\n",
    "    print (cmd)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "onedisk\n",
      "mkdir onedisk\n",
      "echo -------------------------------\n",
      "echo make zfs\n",
      "echo -------------------------------\n",
      "zpool create tank -o ashift=12  sdc\n",
      "echo -------------------------------\n",
      "echo test zfs\n",
      "echo -------------------------------\n",
      "fio --directory=/tank --direct=1 --rw=write --bs=32m --ioengine=libaio --iodepth=64 --filesize=100g --group_reporting --name=throughput-test --eta-newline=1 >> onedisk/zfs_write.txt\n",
      "fio --filename=/tank/throughput-test.0.0 --direct=1 --rw=read --bs=32m --ioengine=libaio --iodepth=64 --filesize=100g --group_reporting --name=throughput-test --eta-newline=1 --readonly >> onedisk/zfs_read.txt\n",
      "rm /tank/throughput-test.0.0\n",
      "echo -------------------------------\n",
      "echo make lustre\n",
      "echo -------------------------------\n",
      "mkfs.lustre --backfstype=zfs --fsname=test --index=0 --reformat --mgs --mdt mgsmdt/mgsmdt\n",
      "mkfs.lustre --backfstype=zfs --fsname=test --index=0 --ost --mgsnode=192.168.169.207@tcp tank/ost0\n",
      "mount -t lustre mgsmdt/mgsmdt /mnt/mgsmdt\n",
      "mount -t lustre tank/ost0 /mnt/ost\n",
      "mount -t lustre 192.168.169.207@tcp:/test /mnt/lustre\n",
      "fio --directory=/mnt/lustre --direct=1 --rw=write --bs=32m --ioengine=libaio --iodepth=64 --filesize=100g --group_reporting --name=throughput-test --eta-newline=1 >> onedisk/lustre_write.txt\n",
      "fio --filename=/mnt/lustre/throughput-test.0.0 --direct=1 --rw=read --bs=32m --ioengine=libaio --iodepth=64 --filesize=100g --group_reporting --name=throughput-test --eta-newline=1 --readonly >> onedisk/lustre_read.txt\n",
      "rm /mnt/lustre/throughput-test.0.0\n",
      "umount -f 192.168.169.207@tcp:/test\n",
      "umount mgsmdt/mgsmdt\n",
      "umount tank/ost0\n",
      "tunefs.lustre --writeconf mgsmdt/mgsmdt\n",
      "tunefs.lustre --writeconf tank/ost0\n",
      "zpool destroy -f tank\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cmds = make_commands(startdisk_, numberdisks_, zfsname_, raidmode_, raid0_, ashift_, compression_, recordsize_, atime_, testname_, \"192.168.169.207\", filesize_, runtime_)\r\n",
    "for cmd in cmds:\r\n",
    "    print (cmd)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mkdir onedisk\n",
      "echo -------------------------------\n",
      "echo make zfs\n",
      "echo -------------------------------\n",
      "zpool create tank -o ashift=12  sdc sdd\n",
      "echo -------------------------------\n",
      "echo test zfs\n",
      "echo -------------------------------\n",
      "fio --directory=/tank --direct=1 --rw=write --bs=32m --ioengine=libaio --iodepth=64 --filesize=100g --group_reporting --name=throughput-test --eta-newline=1 >> onedisk/zfs_write.txt\n",
      "fio --filename=/tank/throughput-test.0.0 --direct=1 --rw=read --bs=32m --ioengine=libaio --iodepth=64 --filesize=100g --group_reporting --name=throughput-test --eta-newline=1 --readonly >> onedisk/zfs_read.txt\n",
      "rm /tank/throughput-test.0.0\n",
      "echo -------------------------------\n",
      "echo make lustre\n",
      "echo -------------------------------\n",
      "mkfs.lustre --backfstype=zfs --fsname=test --index=0 --reformat --mgs --mdt mgsmdt/mgsmdt\n",
      "mkfs.lustre --backfstype=zfs --fsname=test --index=0 --ost --mgsnode=192.168.169.207@tcp tank/ost0\n",
      "mount -t lustre mgsmdt/mgsmdt /mnt/mgsmdt\n",
      "mount -t lustre tank/ost0 /mnt/ost\n",
      "mount -t lustre 192.168.169.207@tcp:/test /mnt/lustre\n",
      "fio --directory=/mnt/lustre --direct=1 --rw=write --bs=32m --ioengine=libaio --iodepth=64 --filesize=100g --group_reporting --name=throughput-test --eta-newline=1 >> onedisk/lustre_write.txt\n",
      "fio --filename=/mnt/lustre/throughput-test.0.0 --direct=1 --rw=read --bs=32m --ioengine=libaio --iodepth=64 --filesize=100g --group_reporting --name=throughput-test --eta-newline=1 --readonly >> onedisk/lustre_read.txt\n",
      "rm /mnt/lustre/throughput-test.0.0\n",
      "umount -f 192.168.169.207@tcp:/test\n",
      "umount mgsmdt/mgsmdt\n",
      "umount tank/ost0\n",
      "tunefs.lustre --writeconf mgsmdt/mgsmdt\n",
      "tunefs.lustre --writeconf tank/ost0\n",
      "zpool destroy -f tank\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# make sure the change device, name, rw\r\n",
    "\"fio --directory=\" \"--direct=1 --rw=write --bs=32m --ioengine=libaio --iodepth=64 --filesize=100g --runtime=120 --time_based --group_reporting --name=throughput-test --eta-newline=1\"\r\n",
    "\"fio --directory=\" \"--direct=1 --rw=read --bs=32m --ioengine=libaio --iodepth=64 --filesize=100g --runtime=120 --time_based --group_reporting --name=throughput-test --eta-newline=1 --readonly\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fio --directory=/tank --direct=1 --rw=write --bs=32m --ioengine=libaio --iodepth=64 --filesize=30g --numjobs=2 --group_reporting --name=throughput-test --eta-newline=1\r\n",
    "\r\n",
    "./bench_fio --target /tank --type directory -o onedisk_fioplot -b 16m 32m 64m -m write --loops 1 --iodepth 1 2 --numjobs 1 2 --ss iops:10% --ss-ramp 10 --ss-dur 30 --runtime 300 --size 20g\r\n",
    "\r\n",
    "./fio_plot/fio_plot -T \"title\" -r write -g -t bw -d 1 2 -n 1 2 -i benchmark_script/onedisk_fioplot/tank/32m/"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit"
  },
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}